{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "IXopHxQnIJbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d82489-77d1-467f-ebc9-fee25ede421e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ukDxFNQ3mNDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae17c433-babc-4363-a3ef-37c9bd7c02b7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "Ekf2ksXHmSqO"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "LIQLYyiumVWj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d zaraks/pascal-voc-2007"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzFlDPDcm1Tw",
        "outputId": "2b9dfa4a-cdac-402b-b760-fa8524f764b8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pascal-voc-2007.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip pascal-voc-2007.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQuCb_ZCnL9Q",
        "outputId": "ac24531a-4abe-4dec-df39-a66dfce297cd"
      },
      "execution_count": 76,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  pascal-voc-2007.zip\n",
            "replace PASCAL_VOC/PASCAL_VOC/pascal_test2007.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "1SpIhexQoE26"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "qIYfoP9fqnMx"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
        "    transforms.Resize(256),  # Resize image to 256x256 (adjust as needed)\n",
        "    #transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.2,0.2,0.2])\n",
        "])"
      ],
      "metadata": {
        "id": "Vw_r4fAtrE2I"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_dataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_dir = os.path.join(root_dir, \"JPEGImages\")\n",
        "        self.mask_dir = os.path.join(root_dir, \"Annotations\")\n",
        "        self.images = os.listdir(self.image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_name = os.path.join(self.mask_dir, self.images[idx])  # Assuming mask has same name as image\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        mask = Image.open(mask_name).convert(\"L\")  # Convert to grayscale\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "aXBK42xeusLE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007\"\n",
        "test_dir = \"/content/VOCtest_06-Nov-2007/VOCdevkit/VOC2007\"\n",
        "\n",
        "#using test dataset for validation purposes"
      ],
      "metadata": {
        "id": "DmnLc3d-qzJ3"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "train_dataset = Custom_dataset(root_dir=train_dir, transform=transform)\n",
        "test_dataset = Custom_dataset(root_dir=test_dir, transform=transform)\n",
        "\n",
        "#dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "JcUCfWh5vqpC"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegNet(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels=3, pretrained=True):\n",
        "        super(SegNet, self).__init__()\n",
        "        vgg_bn = models.vgg16_bn(pretrained=pretrained)\n",
        "        encoder_layers = list(vgg_bn.features.children())\n",
        "        if in_channels != 3:\n",
        "            encoder_layers[0] = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.encoder_stages = nn.ModuleList()\n",
        "        self.decoder_stages = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        decoder_layers = list(reversed([layer for layer in encoder_layers if not isinstance(layer, nn.MaxPool2d)]))\n",
        "        decoder_layers[-1] = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        for idx in range(5):\n",
        "            self.encoder_stages.append(nn.Sequential(*encoder_layers[idx*7: (idx+1)*7]))\n",
        "            self.decoder_stages.append(nn.Sequential(*decoder_layers[idx*7: (idx+1)*7]))\n",
        "\n",
        "        self.final_decoder_conv = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "        for stage in self.encoder_stages:\n",
        "            x, indices = self.pool(stage(x))\n",
        "            encoder_outputs.append((x, indices))\n",
        "\n",
        "        for idx, stage in enumerate(self.decoder_stages):\n",
        "            x, indices = encoder_outputs.pop()\n",
        "            x = self.unpool(x, indices=indices, output_size=encoder_outputs[-1][0].shape[-2:]) if encoder_outputs else x\n",
        "            x = stage(x)\n",
        "\n",
        "        x = self.final_decoder_conv(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7Cq79gwkpxp4"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "num_classes = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 4\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "fczlQWo4woaY"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = SegNet(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "VLwzW6lQwqQU"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks.squeeze(1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_pixels += (predicted == masks).sum().item()\n",
        "        total_pixels += torch.numel(masks)\n",
        "\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "    train_pa = correct_pixels / total_pixels #pixel accuracy\n",
        "\n",
        "    #evaluating on test data\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            test_loss += criterion(outputs, masks.squeeze(1)).item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_pixels += (predicted == masks).sum().item()\n",
        "            total_pixels += torch.numel(masks)\n",
        "\n",
        "    #test_loss /= len(test_loader)\n",
        "    #test_pa = correct_pixels / total_pixels\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.3f}, Train Pixel Accuracy: {train_pa:.3f}\")\n",
        "\n",
        "    train_losses.append(train_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "WTzUNqDjw3Xy",
        "outputId": "bd47d0a9-3f7e-49fb-d482-be518c72270e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/003783.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-1364cf703795>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-fe7db3632889>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/003783.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting training loss vs epoch curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss vs Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DAsI_gkJxR9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#part 2 - computing various segmentation metrics\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        #flatten tensors to compute metrics\n",
        "        true_labels.extend(masks.cpu().numpy().flatten())\n",
        "        predicted_labels.extend(predicted.cpu().numpy().flatten())\n",
        "\n",
        "pixel_accuracy = accuracy_score(true_labels, predicted_labels) #pixel accuracy\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels) #confusion matrix\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted') #F1 score\n",
        "\n",
        "#iou\n",
        "iou_per_class = []\n",
        "for i in range(num_classes):\n",
        "    intersection = np.logical_and(true_labels == i, predicted_labels == i).sum()\n",
        "    union = np.logical_or(true_labels == i, predicted_labels == i).sum()\n",
        "    iou_per_class.append(intersection / union)\n",
        "\n",
        "mean_iou = np.mean(iou_per_class)\n",
        "\n",
        "print(f\"Pixel Accuracy: {pixel_accuracy:.4f}\")\n",
        "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "QvHeRNWJzpIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#facing issues in finding the masked images for the dataset - they have different names"
      ],
      "metadata": {
        "id": "xqgr6u4E9ozi"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3 - changing backbone of segnet to lighter model. Therefore, using, simple cnn instead."
      ],
      "metadata": {
        "id": "lfXUC8mr5PhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels=3):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
        "        self.upconv2 = nn.ConvTranspose2d(16, num_classes, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.upconv1(x))\n",
        "        x = F.relu(self.upconv2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "sqQ-3Xny2TRs"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class segnet_CNN(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels=3):\n",
        "        super(segnet_CNN, self).__init__()\n",
        "        self.cnn = CNN(num_classes=num_classes, in_channels=in_channels)\n",
        "\n",
        "        self.encoder_stages = nn.ModuleList()\n",
        "        self.decoder_stages = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        for _ in range(5):\n",
        "            self.encoder_stages.append(nn.Sequential(nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "                                                      nn.ReLU(inplace=True)))\n",
        "            self.decoder_stages.append(nn.Sequential(nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "                                                      nn.ReLU(inplace=True)))\n",
        "\n",
        "        self.final_decoder_conv = nn.Conv2d(32, num_classes, kernel_size=3, padding=1)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "        for stage in self.encoder_stages:\n",
        "            x, indices = self.pool(stage(x))\n",
        "            encoder_outputs.append((x, indices))\n",
        "\n",
        "        for idx, stage in enumerate(self.decoder_stages):\n",
        "            x, indices = encoder_outputs.pop()\n",
        "            x = self.unpool(x, indices=indices, output_size=encoder_outputs[-1][0].shape[-2:]) if encoder_outputs else x\n",
        "            x = stage(x)\n",
        "\n",
        "        x = self.final_decoder_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Su5p9z1C5pZ3"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numclasses = 2\n",
        "epochs = 10\n",
        "\n",
        "model_2 = segnet_CNN(num_classes=numclasses).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_2 = nn.CrossEntropyLoss()\n",
        "optimizer_2 = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "WKaY6z_C6Aza"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct_pixels = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        outputs = model_2(images)\n",
        "        loss = criterion_2(outputs, masks.squeeze(1))\n",
        "\n",
        "        optimizer_2.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_2.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_pixels += (predicted == masks).sum().item()\n",
        "        total_pixels += torch.numel(masks)\n",
        "\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "    train_pixel_accuracy = correct_pixels / total_pixels\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.3f}, Train Pixel Accuracy: {train_pixel_accuracy:.3f}\")\n"
      ],
      "metadata": {
        "id": "GjwIPm9E9nfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        true_labels.extend(masks.cpu().numpy().flatten())\n",
        "        predicted_labels.extend(predicted.cpu().numpy().flatten())\n",
        "\n",
        "pixel_accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Pixel Accuracy: {pixel_accuracy:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "d5p339_G6cqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}