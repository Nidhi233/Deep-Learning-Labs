{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5hmX4JgvcOtJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data transformation\n",
        "transform_train = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "#importing cifar10 dataset\n",
        "train_dataset = datasets.CIFAR100(root='data', train=True, transform=transform_train, download=True)\n",
        "test_dataset = datasets.CIFAR100(root='data', train=False, transform=transform_test)\n",
        "\n",
        "#dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h0goGOUcWKI",
        "outputId": "ffac493e-3716-45a0-bce5-b5772a6644c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 66740590.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining dense block in DenseNet\n",
        "'''\n",
        "Each layer consists of batch normalization, ReLU activation, depthwise convolution and pointwise convolution.\n",
        "Forward pass concatenates the outputs of each layer and next one recieves features from all the preceding layers.\n",
        "'''\n",
        "class DenseBlock(nn.Module):\n",
        "  def __init__(self, in_channels, growth_rate):\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    for _ in range(4):  # Adjust number of layers as needed\n",
        "        layers.extend([\n",
        "          nn.BatchNorm2d(in_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1,bias=False),\n",
        "        ])\n",
        "        in_channels += growth_rate\n",
        "\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "    features = [x]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x=layer(x)\n",
        "      features.append(x)\n",
        "      x=torch.cat(features,dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "8zxWaVGccjE5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transition layer (downsamples features -- reducing number of channels and spatial dimension while maintaining info flow)\n",
        "class TransitionLayer(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.convl = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.convl(x)\n",
        "    x=self.pool(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "45utsMB1cqLu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet(nn.Module):\n",
        "  def __init__(self, growth_rate=8, num_blocks=[6,12,24,32], depthwise_conv=True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.convl2=nn.Conv2d(3, growth_rate*2, kernel_size=3)\n",
        "    self.dense_blocks=nn.ModuleList()\n",
        "\n",
        "    in_channels=growth_rate*2\n",
        "    for num_layers in num_blocks:\n",
        "      self.dense_blocks.append(DenseBlock(in_channels, growth_rate))\n",
        "      in_channels += num_layers * growth_rate\n",
        "      self.dense_blocks.append(TransitionLayer(in_channels, in_channels//2))\n",
        "      in_channels //= 2\n",
        "\n",
        "    self.pool = nn.AvgPool2d(kernel_size=8, stride=8)\n",
        "    self.fc = nn.Linear(in_channels, 100)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convl2(x)\n",
        "\n",
        "    for block in self.dense_blocks:\n",
        "       x = block(x)\n",
        "\n",
        "    x = self.pool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "7qdl3XzhctTN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DenseNet(depthwise_conv=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "jIja7zricwmi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "epochs = 30\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  #validation\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  val_loss /= len(test_loader)\n",
        "  val_losses.append(val_loss)\n"
      ],
      "metadata": {
        "id": "_MTIwKxVczzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "def evaluate(model, test_loader, device):\n",
        "  model.eval()\n",
        "  all_pred = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      _, predictions = torch.max(outputs, 1)\n",
        "      all_pred.extend(predictions.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  return all_pred, all_labels"
      ],
      "metadata": {
        "id": "ltrCFr4pc2Hh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating evaluation metrices\n",
        "predictions, labels = evaluate(model, test_loader, device)\n",
        "\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "precision = precision_score(labels, predictions, average='weighted')\n",
        "recall = recall_score(labels, predictions, average='weighted')\n",
        "f1 = f1_score(labels, predictions, average='weighted')"
      ],
      "metadata": {
        "id": "rHq0XOiijEqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "L3ukwbqQiO7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without dense connections (will be building second model - adding *Without* as prefix)"
      ],
      "metadata": {
        "id": "U-QyebD4fghx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WithoutDenseBlock(nn.Module):\n",
        "  def __init__(self, in_channels, growth_rate):\n",
        "      super(WithoutDenseBlock, self).__init__()\n",
        "\n",
        "      self.layers = nn.ModuleList()\n",
        "      for _ in range(4):\n",
        "        self.layers.append(nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, groups=in_channels, padding=1),\n",
        "            nn.Conv2d(in_channels, growth_rate, kernel_size=1)\n",
        "        ))\n",
        "        in_channels += growth_rate\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = x\n",
        "\n",
        "      for layer in self.layers:\n",
        "        out = layer(out)\n",
        "        x = x + out #simple addition\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "ULiNfrEedk0X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WithoutDenseNet(nn.Module):\n",
        "  def __init__(self, growth_rate=12, num_blocks=[6, 12, 24, 32]):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, growth_rate*2, kernel_size=3)\n",
        "    self.dense_blocks = nn.ModuleList()\n",
        "\n",
        "    in_channels = growth_rate*2\n",
        "    for num_layers in num_blocks:\n",
        "      self.dense_blocks.append(WithoutDenseNet(in_channels, growth_rate))\n",
        "      in_channels += num_layers * growth_rate\n",
        "      self.dense_blocks.append(TransitionLayer(in_channels, in_channels//2))\n",
        "      in_channels //= 2\n",
        "\n",
        "    self.pool = nn.AvgPool2d(kernel_size=8, stride=8)\n",
        "    self.fc = nn.Linear(in_channels, 100)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      for block in self.dense_blocks:\n",
        "        x = block(x)\n",
        "      x = self.pool(x)\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = self.fc(x)\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "Jd4BMxx5gn7S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = WithoutDenseNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "1vGAngogipQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "epochs = 30\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  #validation\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  val_loss /= len(test_loader)\n",
        "  val_losses.append(val_loss)\n"
      ],
      "metadata": {
        "id": "9ef6teiJhZio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "def evaluate(model, test_loader, device):\n",
        "  model.eval()\n",
        "  all_pred = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      _, predictions = torch.max(outputs, 1)\n",
        "      all_pred.extend(predictions.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  return all_pred, all_labels"
      ],
      "metadata": {
        "id": "4wBqb7T3hc70"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating evaluation metrices\n",
        "predictions, labels = evaluate(model, test_loader, device)\n",
        "\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "precision = precision_score(labels, predictions, average='weighted')\n",
        "recall = recall_score(labels, predictions, average='weighted')\n",
        "f1 = f1_score(labels, predictions, average='weighted')\n"
      ],
      "metadata": {
        "id": "70KBt-FMhiq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-b1sgeXdCsQ",
        "outputId": "9e32612a-3e06-4e5f-8d42-88c69655768c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6356\n",
            "Precision: 0.6481\n",
            "Recall: 0.6356\n",
            "F1 Score: 0.6297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN model (consisting of depthwise+pointwise) works better when used along with dense network."
      ],
      "metadata": {
        "id": "vpLPo1R2lP3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}